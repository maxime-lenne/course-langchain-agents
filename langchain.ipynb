{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f4459b",
   "metadata": {},
   "source": [
    "![LangChain](img/langchain.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511a0f0",
   "metadata": {},
   "source": [
    "Les **Agents** dans LangChain ouvrent la voie √† des syst√®mes plus dynamiques, capables de **raisonner √©tape par √©tape** et d‚Äô**interagir avec des outils** pour accomplir des t√¢ches complexes.\n",
    "\n",
    "Contrairement aux cha√Ænes statiques (chains), **‚ö†Ô∏è un agent ne suit pas un chemin pr√©d√©fini**. **Il s‚Äôappuie sur un LLM** qui d√©cide dynamiquement, √† chaque √©tape, quelle action entreprendre : quel outil utiliser, quelles informations rechercher ou comment poursuivre, en fonction du contexte.\n",
    "\n",
    "Les outils, ou **tools**, sont des fonctions encapsul√©es que l‚Äôagent peut appeler, il peut s'agir de fonctions pour interroger une base de donn√©es, de consulter une API, ou d‚Äôex√©cuter un calcul.\n",
    "\n",
    "**Gr√¢ce √† cette combinaison :**\n",
    "\n",
    "> raisonnement du LLM ‚Üí proposition d‚Äôaction ‚Üí ex√©cution par l‚Äôagent ‚Üí observation ‚Üí nouveau raisonnement ‚Üí et ainsi de suite...\n",
    "\n",
    "... un agent LangChain devient un orchestrateur intelligent, capable de r√©soudre des probl√®mes ouverts ou de r√©pondre √† des requ√™tes complexes, sans suivre un script rigide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb160db",
   "metadata": {},
   "source": [
    "![Agent](img/agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3075ae",
   "metadata": {},
   "source": [
    "L‚Äôagent suit un **sch√©ma it√©ratif** bas√© sur le pattern **ReAct (Reasoning + Acting)**.  \n",
    "√Ä partir d‚Äôune requ√™te, l'agent interagit avec un mod√®le de langage (LLM) qui raisonne √©tape par √©tape (**Thought**) et propose des actions (**Action**) √† effectuer √† l‚Äôaide d‚Äôoutils disponibles.   \n",
    "L‚Äôagent ex√©cute ces actions, collecte les r√©sultats (**Observation**), et les renvoie au LLM pour affiner son raisonnement.   \n",
    "Ce cycle **ReAct** se r√©p√®te jusqu‚Äô√† ce que le LLM formule une r√©ponse finale (**Final Answer**), que l‚Äôagent retourne √† l‚Äôutilisateur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738a13fb",
   "metadata": {},
   "source": [
    "![Hugging Face](img/hugging_face.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7eb01a",
   "metadata": {},
   "source": [
    "# 1. Chargement du mod√®le LLM local\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafb689",
   "metadata": {},
   "source": [
    "Dans cette section, nous chargeons un mod√®le de langage local gr√¢ce √† **Ollama**. Cela permet de travailler avec un **LLM directement sur notre machine**, sans connexion √† une API externe.\n",
    "\n",
    "Nous utilisons ici la classe `ChatOllama` de **LangChain**, qui nous permet d‚Äôinteragir facilement avec un mod√®le comme **llama3** d√©j√† t√©l√©charg√© via Ollama."
   ]
  },
  {
   "cell_type": "code",
   "id": "7301c899",
   "metadata": {},
   "source": [
    "import os\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain import hub\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Chargement des cl√©s d'API se trouvant dans le fichier .env.  \n",
    "# Ceci permet d'utiliser des mod√®les en ligne comme gpt-x, deepseek-x, etc...\n",
    "load_dotenv(override=True)\n",
    "\n",
    "model = ChatOllama(model=\"llama3\", temperature=0)\n",
    "#model = ChatDeepSeek(model=\"deepseek-chat\", api_key=os.getenv(\"DEEPSEEK_API_KEY\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a48a0cd4",
   "metadata": {},
   "source": [
    "# 2. Agent standard\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce1aa8",
   "metadata": {},
   "source": [
    "Un agent standard permet d‚Äôutiliser un mod√®le de langage avec un ou plusieurs **outils externes**, comme des fonctions Python, pour r√©pondre √† une t√¢che sp√©cifique.  \n",
    "Cet agent **fonctionne sans m√©moire** : il ne conserve **aucun historique des interactions pr√©c√©dentes**. Chaque question est trait√©e de mani√®re ind√©pendante, comme une **requ√™te isol√©e**.\n",
    "\n",
    "Dans cet exemple, l‚Äôagent utilise un outil simple pour r√©pondre √† la question ¬´ Quelle heure est-il ? ¬ª, en appelant une fonction qui retourne l‚Äôheure actuelle.\n",
    "Son comportement est guid√© par un prompt ReAct standard charg√© depuis LangChain Hub, qui lui permet de raisonner et de d√©cider quand utiliser un outil."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91f33b1",
   "metadata": {},
   "source": [
    "### 2.1 Pr√©paration des outils"
   ]
  },
  {
   "cell_type": "code",
   "id": "50c53956",
   "metadata": {},
   "source": [
    "# D√©finition de l'outil : retourne l'heure actuelle au format HH:MM\n",
    "def get_current_time(*args, **kwargs):\n",
    "    return datetime.now().strftime(\"%H:%M\")\n",
    "\n",
    "# Liste des outils que l'agent peut utiliser. Chaque outil est expos√© au LLM via un nom et une description.\n",
    "# Cela permet au LLM, durant son raisonnement, de d√©cider quel outil appeler en fonction de la t√¢che √† accomplir.\n",
    "# Ici, un seul outil est d√©fini : \"CurrentTime\", qui retourne l'heure actuelle au format HH:MM.\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"CurrentTime\",\n",
    "        func=get_current_time,\n",
    "        description=\"Use this tool to get the current time.\"\n",
    "    )\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "09e15c58",
   "metadata": {},
   "source": [
    "### 2.2 Pr√©paration et usage de l'agent"
   ]
  },
  {
   "cell_type": "code",
   "id": "e5c311b6",
   "metadata": {},
   "source": [
    "# Chargement du prompt standard pour le paradigme ReAct depuis LangChain Hub\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "# Cr√©ation de l'agent ReAct avec le mod√®le, les outils et le prompt\n",
    "agent = create_react_agent(\n",
    "    llm=model,\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# Encapsulation de l‚Äôagent dans un ex√©cuteur avec configuration de contr√¥le\n",
    "executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True    # Affiche les √©tapes de raisonnement (utile en debug)\n",
    ")\n",
    "\n",
    "# Lancement de l‚Äôagent avec une question en entr√©e\n",
    "response = executor.invoke({\"input\": \"Quelle heure est-il ?\"})\n",
    "\n",
    "display(Markdown(response[\"output\"]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c9bbeab8",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909bdabb",
   "metadata": {},
   "source": [
    "> Exercice 1\n",
    "\n",
    "Cr√©ez un agent capable de faire des conversions de temp√©rature. Votre agent doit pouvoir r√©pondre √† des questions comme :\n",
    "- *‚ÄúQuelle est la temp√©rature en Celsius pour 100 Fahrenheit ?‚Äù*\n",
    "- *‚ÄúConvertis 37.5 degr√©s Celsius en Fahrenheit.‚Äù*\n",
    "\n",
    "üí° Utilisez 2 **tools** diff√©rents\n",
    "\n",
    "üí™üèª Bonus : Autoriser des entr√©es plus souples, comme ‚ÄúConvertis 100 F en C‚Äù ou ‚ÄúCelsius pour 212¬∞F‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "id": "7b977196",
   "metadata": {},
   "source": [
    "# Votre code ici"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f45e9a8",
   "metadata": {},
   "source": [
    "# 2. Agent conversationnel\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0c71cf",
   "metadata": {},
   "source": [
    "Un agent conversationnel est con√ßu pour g√©rer un **dialogue continu**, en conservant une **m√©moire des √©changes pr√©c√©dents**. Contrairement √† l‚Äôagent standard qui traite chaque requ√™te ind√©pendamment, un agent conversationnel **peut adapter ses r√©ponses en fonction du contexte accumul√© dans la conversation**.\n",
    "\n",
    "Ce type d‚Äôagent est particuli√®rement utile pour construire des assistants interactifs, des conseillers ou des syst√®mes de FAQ qui doivent s‚Äôadapter aux intentions de l‚Äôutilisateur au fil du temps.\n",
    "\n",
    "LangChain permet d‚Äôajouter une m√©moire √† un agent gr√¢ce √† des modules comme `ConversationBufferMemory`, afin que le mod√®le de langage puisse prendre en compte l‚Äôhistorique des √©changes lors de chaque nouvelle interaction."
   ]
  },
  {
   "cell_type": "code",
   "id": "3a3486d3",
   "metadata": {},
   "source": [
    "# R√©cup√©ration d‚Äôun prompt conversationnel React (bas√© sur ReAct)\n",
    "prompt = hub.pull(\"hwchase17/react-chat\")\n",
    "\n",
    "# Initialisation de la m√©moire pour suivre l‚Äôhistorique des √©changes\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Cr√©ation de l'agent ReAct avec le mod√®le, les outils et le prompt\n",
    "agent = create_react_agent(\n",
    "    llm=model,\n",
    "    tools=tools,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# Construction d‚Äôun ex√©cuteur qui lie l‚Äôagent, les outils et la m√©moire\n",
    "executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    memory=memory,\n",
    "    verbose=True    # Affiche les √©tapes de raisonnement (utile en debug)\n",
    ")\n",
    "\n",
    "# Boucle interactive terminale\n",
    "while True:\n",
    "    user_input = input(\"Vous : \")\n",
    "    clear_output(wait=True)                         # Efface l'affichage pr√©c√©dent\n",
    "    display(Markdown(f\"**Vous :** {user_input}\"))   # Affiche la requ√™te de l'utilisateur\n",
    "\n",
    "    if user_input.lower() in [\"stop\", \"exit\", \"quit\"]:\n",
    "        print(\"Fin de la conversation.\")\n",
    "        break\n",
    "\n",
    "    response = executor.invoke({\"input\": user_input})\n",
    "    display(Markdown(response[\"output\"]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fad82676",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5abd6bb",
   "metadata": {},
   "source": [
    "> Exercice 1\n",
    "\n",
    "Reprenez vos travaux sur l'exercice pr√©c√©dent pour y introduire un aspect conversationnel gr√¢ce √† une boucle de conversation et √† la gestion de la m√©moire (`ConversationBufferMemory`)."
   ]
  },
  {
   "cell_type": "code",
   "id": "f35cb257",
   "metadata": {},
   "source": [
    "# Votre code ici"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
